import os
import hashlib

from pathlib import Path
from collections import Counter
from typing import List

# Agno / Phidata imports
from agno.vectordb.chroma import ChromaDb
from agno.knowledge.embedder.vllm import VLLMEmbedder
from agno.knowledge.embedder.jina import JinaEmbedder
from agno.knowledge.document.base import Document

from agno.tools import Toolkit
from dotenv import load_dotenv

# AST Splitting imports
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language

class CodeVectorStore(Toolkit):
    def __init__(self):
        super().__init__(name="build_vector_base", tools=[self.build_vector_base])

        self.vector_db_dir = "../Knowledge/vector_db"  # ç›´æ¥å®šæ­»ï¼Œä¸è¦ç»™æ¨¡å‹æ”¹ã€‚å»ºè®®é…ç½®ç»å¯¹è·¯å¾„ã€‚

        self.api_key = os.getenv("JINA_API_KEY")
        assert self.api_key is not None, "JINA_API_KEY is not set"


    # ç”¨æ¥ç»Ÿè®¡ä»£ç åº“çš„è¯­è¨€ï¼Œä»¥ç¡®å®šåˆ†chunkæ–¹å¼ã€‚
    def _map_ext_to_lang(self, ext: str) -> Language:
        """æ‰©å±•å -> LangChain Language æšä¸¾æ˜ å°„"""
        mapping = {
            ".py": Language.PYTHON,
            ".js": Language.JS,
            ".jsx": Language.JS,
            ".ts": Language.JS,  # TS é€šå¸¸å…¼å®¹ JS åˆ‡åˆ†é€»è¾‘
            ".tsx": Language.JS,
            ".java": Language.JAVA,
            ".go": Language.GO,
            ".cpp": Language.CPP,
            ".c": Language.CPP,
            ".h": Language.CPP,
            ".cs": Language.CSHARP,
            ".rs": Language.RUST,
            ".md": Language.MARKDOWN,
            ".sol": Language.SOL, # Solidity
            ".rb": Language.RUBY,
            ".php": Language.PHP,
        }
        return mapping.get(ext, Language.PYTHON) # æœªçŸ¥ç±»å‹é»˜è®¤ç”¨Python


    def _get_files(self) -> List[str]:
        """[å†…éƒ¨æ–¹æ³•] é«˜æ€§èƒ½è·å–ä»“åº“ä»£ç æ–‡ä»¶ (O(1) éå† + å‰ªæ + ç†”æ–­)"""
        
        # 1. ä½¿ç”¨ Set è¿›è¡Œ O(1) æŸ¥æ‰¾ï¼Œæ¯” List å¿«
        # åŒ…å«äº†å¸¸è§çš„ä»£ç å’Œæ–‡æ¡£æ ¼å¼
        target_extensions = {
            ".py", ".js", ".ts", ".java", ".go", ".cpp", ".c", ".h", ".rs", 
            ".md", ".txt", ".rst", ".yaml", ".yml", ".json"
        }
        
        # 2. ç›®å½•é»‘åå• - åªè¦é‡åˆ°è¿™äº›ç›®å½•ï¼Œç›´æ¥è·³è¿‡ï¼Œç»ä¸è¿›å…¥
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ç›®å½•åï¼Œè€Œä¸æ˜¯å®Œæ•´è·¯å¾„
        ignore_dirs = {
            'node_modules', 'dist', 'build', 'venv', '.git', '.idea', '.vscode', 
            '__pycache__', 'migrations', 'target', 'bin', 'obj', 'lib', 'vendor', 
            'docker'
        }
        
        # 3. å•æ–‡ä»¶å¤§å°é˜ˆå€¼ (ä¾‹å¦‚ 100KB)
        # è¶…è¿‡è¿™ä¸ªå¤§å°çš„ä»£ç æ–‡ä»¶é€šå¸¸æ˜¯æœºå™¨ç”Ÿæˆçš„
        max_file_size_bytes = 100 * 1024 
        all_files, ext_counter = [], Counter()
        
        # os.walk åªéœ€è¦éå†ä¸€æ¬¡ç£ç›˜
        for root, dirs, files in os.walk(self.repo_path):
            # [å…³é”®ä¼˜åŒ–]ï¼šåŸåœ°ä¿®æ”¹ dirs åˆ—è¡¨
            # os.walk ä¼šæ ¹æ® dirs åˆ—è¡¨å†³å®šä¸‹ä¸€æ­¥è¿›å…¥å“ªäº›å­ç›®å½•ã€‚
            dirs[:] = [d for d in dirs if d not in ignore_dirs and not d.startswith('.')]
            
            for file in files:
                # 1. æ‰©å±•åå¿«é€Ÿæ£€æŸ¥
                _, ext = os.path.splitext(file)
                if ext not in target_extensions:
                    continue
                
                # 2. æ’é™¤éšè—æ–‡ä»¶
                if file.startswith('.'):
                    continue
                full_path = os.path.join(root, file)
                
                # 3. æ–‡ä»¶å¤§å°æ£€æŸ¥
                # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œstat ä¹Ÿæ˜¯ç¬é—´å®Œæˆçš„ï¼Œæ¯”è¯»å–å†…å®¹å¿«å¾—å¤š
                try:
                    file_size = os.path.getsize(full_path)
                    if file_size > max_file_size_bytes:
                        # å¯ä»¥åœ¨è¿™é‡Œæ‰“å°ä¸ª logï¼Œæ–¹ä¾¿ debug
                        # print(f"Skipping large file: {file} ({file_size/1024:.2f} KB)")
                        continue
                except OSError:
                    continue
                
                ext_counter[ext] += 1
                all_files.append(full_path)
            
        if ext_counter:
            # è·å–å‡ºç°æ¬¡æ•°æœ€å¤šçš„åç¼€ï¼Œä¾‹å¦‚: ('.py', 150)
            most_common_ext, count = ext_counter.most_common(1)[0]
            
            # æ˜ å°„ä¸º Language æšä¸¾
            self._language = self._map_ext_to_lang(most_common_ext)
            
            # è®¡ç®—å æ¯”ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—ï¼‰
            total_valid_files = sum(ext_counter.values())
            ratio = count / total_valid_files
            
            print(f"ğŸ“Š Language Detection: Main='{self._language.value}' "
                f"(Based on {most_common_ext}: {count}/{total_valid_files} files, {ratio:.1%})")
        
        else:
            print("âš ï¸ No valid code files found, defaulting to Python.")
            self._language = Language.PYTHON
                

        print(f"âœ… æ–‡ä»¶æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_files)} ä¸ªæœ‰æ•ˆä»£ç æ–‡ä»¶ã€‚")
        return all_files

    def _load_and_split_ast(self) -> List[Document]:
        """æ ¸å¿ƒé€»è¾‘ï¼šåŠ¨æ€ AST åˆ‡åˆ† + ä¸Šä¸‹æ–‡æ³¨å…¥ + å¤§å—åˆ‡åˆ†ç­–ç•¥"""
        files = self._get_files()  # è·å–ä»£ç æ–‡ä»¶
        documents = []
        
        print(f"ğŸ” Found {len(files)} files. Starting optimized ingestion...")
        # ç»Ÿè®¡æ•°æ®ï¼Œç”¨äºä¼˜åŒ–ã€å»é‡
        total_chunks = 0
        hash_set = set()

        for file_path in files:
            try:
                # 1. è·å–æ–‡ä»¶ç±»å‹
                _, list_ext = os.path.splitext(file_path)
                
                # 2. è¯»å–æ–‡ä»¶
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # 3. é’ˆå¯¹ä¸åŒè¯­è¨€ä½¿ç”¨ä¸“é—¨çš„åˆ†éš”ç¬¦ï¼ˆæ¯”å¦‚ Python ä¼˜å…ˆæŒ‰ def/class åˆ‡ï¼ŒJS æŒ‰ function åˆ‡ï¼‰
                current_lang = self._language
                
                # 4. chunk_size: 2000 (çº¦ 500-800 è¡Œä»£ç )ã€‚æˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æŠŠä¸€ä¸ªå®Œæ•´çš„ Class æˆ–å‡½æ•°æ”¾åœ¨ä¸€ä¸ªå—é‡Œã€‚
                # chunk_overlap: 200ã€‚ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚
                splitter = RecursiveCharacterTextSplitter.from_language(
                    language=current_lang,
                    chunk_size=2000, 
                    chunk_overlap=200
                )
                raw_chunks = splitter.create_documents([content])

                # è¿‡æ»¤æ‰å¤ªçŸ­çš„å†…å®¹ï¼Œä¾‹å¦‚ç©ºè¡Œã€æ³¨é‡Š
                raw_chunks = [raw_chunk.page_content for raw_chunk in raw_chunks if len(raw_chunk.page_content)>=10]
                
                for i, page_content in enumerate(raw_chunks):
                    # å°†è·¯å¾„ä½œä¸ºä¸Šä¸‹æ–‡æ³¨å…¥
                    enriched_content = f"// File: {file_path}\n// Part: {i}\n\n{page_content}"
                    page_hash = hashlib.md5(enriched_content.encode()).hexdigest()
                    if page_hash in hash_set:  # å»é‡
                        continue  # è·³è¿‡
                    else:
                        hash_set.add(page_hash)

                    doc = Document(
                        name=f"{file_path}:{i}",
                        id=f"{self.repo_path}:{file_path}:{i}",
                        content=enriched_content,  # æœç´¢æ—¶ï¼Œæ¨¡å‹èƒ½ç›´æ¥çœ‹åˆ°æ–‡ä»¶å
                        meta_data={
                            # "file_path": self.repo_path,
                            "file_name": file_path,
                            "chunk_index": i,
                            "language": list_ext,
                        }
                    )
                    documents.append(doc)
                    total_chunks += 1
            
            except Exception as e:
                # ç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ logging.warning
                print(f"âš ï¸ Error processing {file_path}: {e}")

        print(f"âœ… AST Split completed. Total vectors generated: {len(documents)}")
        self.vector_db.insert('kek', documents)

        return documents

    def _index_exists(self) -> bool:
        """[å†…éƒ¨æ–¹æ³•] æ£€æŸ¥é›†åˆä¸­æ˜¯å¦å·²ç»æœ‰æ•°æ®"""
        if Path(self.final_path).exists():
            file_count = sum(1 for item in Path(self.final_path).iterdir() if item.is_file())
            if file_count != 1:
                print(
                    f"ç›®æ ‡è·¯å¾„å·²å­˜åœ¨: {self.final_path}ã€‚"
                    f"è¯·åˆ é™¤è¯¥ç›®å½•æˆ–é€‰æ‹©å…¶ä»–è·¯å¾„ã€‚"
                )
                return self.final_path
        

    def build_vector_base(self, repo_path: str) -> str:
        """
        æŒ‰ç…§è¯­æ³•æ ‘è§„åˆ™å¯¹æœ¬åœ°githubä»“åº“çš„ä»£ç è¿›è¡Œåˆ†chunkï¼Œå¹¶æ„å»ºå‘é‡çŸ¥è¯†åº“
        Args:
            repo_path(str): æœ¬åœ°githubä»“åº“çš„ç»å¯¹è·¯å¾„
        Returns:
            str: è¿”å›æœ¬åœ°githubä»“åº“å¯¹åº”çš„å‘é‡åŒ–æ•°æ®åº“çš„ç»å¯¹è·¯å¾„
        """
        if not os.path.exists(repo_path):
            raise ValueError(f"Repository path does not exist: {repo_path}")
        
        self.repo_path = os.path.abspath(repo_path)
        self.repo_name = os.path.basename(self.repo_path)

        self.final_path = os.path.join(self.vector_db_dir, self.repo_name)

        print(f"ğŸš€ Initializing codebase build for: {self.repo_name}")
        
        # ====åˆå§‹åŒ–====
        # 1. é…ç½® Embedding æ¨¡å‹ (Qwen)
        # åŠ¡å¿…ç¡®è®¤ model_id æ­£ç¡®ï¼Œå¦‚æœç»å¸¸ä¸‹è½½æ…¢ï¼Œå»ºè®®ä¸‹è½½åˆ°æœ¬åœ°åå¡«å†™æœ¬åœ°ç»å¯¹è·¯å¾„
        
        self.embedder = JinaEmbedder(
            id="jina-embeddings-v3",
            dimensions=1024,
            embedding_type="float",
            late_chunking=True,
            batch_size=50,
            timeout=30.0, 
            api_key=self.api_key
        )
        """self.embedder = VLLMEmbedder(
            id='Qwen/Qwen3-Embedding-0.6B',  # å»ºè®®é…ç½®ç»å¯¹è·¯å¾„ã€‚
            dimensions=1024, 
            batch_size=512
        ) """

        # 2. é…ç½® ChromaDB (è‡ªåŠ¨æŒä¹…åŒ–åˆ°æ–‡ä»¶å¤¹)
        # å…ˆæ£€æµ‹æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        if self._index_exists():
            print(f"âœ… Knowledge base for '{self.repo_name}' already exists. Skipping build.")
            return str(self.final_path)
        
        # ChromaDB ä¼šåœ¨ persist_dir ä¸‹åˆ›å»º sqlite3 æ–‡ä»¶å’ŒäºŒè¿›åˆ¶ç´¢å¼•
        self.vector_db = ChromaDb(
            collection=self.repo_name, 
            embedder=self.embedder,
            persistent_client=True, # å¯ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
            path=self.final_path # æŒ‡å®šå­˜å‚¨è·¯å¾„
        )
        self.vector_db.create()

        # 2. ç”Ÿæˆæ–‡æ¡£
        print("âš™ï¸  Parsing and splitting code...")
        docs = self._load_and_split_ast()
        
        if not docs:
            print("âš ï¸ No documents generated. Check your repo path.")
            return 'No documents generated.'
        return str(self.final_path)


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    load_dotenv()
    # é…ç½®
    TARGET_REPO = "/workspace/ai-test/AgentPractice/Knowledge/codebase/verl" # æ›¿æ¢ä¸ºä½ çš„ç›®æ ‡ä»“åº“è·¯å¾„
    
    # åˆå§‹åŒ–
    # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨åˆ›å»º ./local_knowledge_db æ–‡ä»¶å¤¹å¹¶åœ¨é‡Œé¢ç”Ÿæˆ chroma.sqlite3
    vector_store = CodeVectorStore()
    
    # æ ¸å¿ƒæ„å»º (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè·‘è¿›åº¦æ¡ï¼Œç¬¬äºŒæ¬¡è¿è¡Œä¼šç›´æ¥è·³è¿‡)
    vector_store.build_vector_base(TARGET_REPO)
